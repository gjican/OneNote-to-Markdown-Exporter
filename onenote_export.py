#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
OneNote to Markdown Exporter

æ­¤è„šæœ¬ä½¿ç”¨ Microsoft Graph API å°† OneNote ç¬”è®°æœ¬å¯¼å‡ºä¸º Markdown æ ¼å¼ï¼Œ
å¹¶è‡ªåŠ¨ä¸‹è½½å›¾ç‰‡å’Œé™„ä»¶ã€‚

Usage:
    python onenote_export.py

Author: gjican
License: MIT
"""

import os
import sys
import json
import re
import time
import requests
import msal
from markdownify import markdownify as md
from bs4 import BeautifulSoup

# é…ç½®éƒ¨åˆ†
# -----------------------------------------------------------------------------
# Microsoft Graph Client ID
# è¿™ä¸ª ID æ˜¯å…¬å¼€çš„ï¼Œæ”¯æŒ Device Code Flowï¼Œé€‚ç”¨äºä¸ªäºº Microsoft è´¦æˆ·ã€‚
# å¦‚æœä½ éœ€è¦ç”¨äºç»„ç»‡/å­¦æ ¡è´¦æˆ·ï¼Œå¯èƒ½éœ€è¦æ³¨å†Œè‡ªå·±çš„ Azure Appã€‚
CLIENT_ID = '14d82eec-204b-4c2f-b7e8-296a70dab67e' 

# è®¤è¯ç«¯ç‚¹
# å¯¹äºä¸ªäººè´¦æˆ·ä½¿ç”¨ 'consumers'ï¼Œå¯¹äºç»„ç»‡è´¦æˆ·é€šå¸¸ä½¿ç”¨ 'organizations' æˆ– 'common'
AUTHORITY = 'https://login.microsoftonline.com/consumers'

# è¯·æ±‚çš„æƒé™èŒƒå›´
SCOPES = ['Notes.Read', 'Notes.Read.All', 'User.Read']

# å¯¼å‡ºç›®å½•åç§°
EXPORT_DIR = "OneNote_Export"
# -----------------------------------------------------------------------------

def get_access_token():
    app = msal.PublicClientApplication(
        CLIENT_ID, 
        authority=AUTHORITY
    )
    
    # ä½¿ç”¨è®¾å¤‡ä»£ç æµ (Device Code Flow)
    flow = app.initiate_device_flow(scopes=SCOPES)
    if 'user_code' not in flow:
        raise ValueError("æ— æ³•åˆå§‹åŒ–è®¾å¤‡ä»£ç æµ: " + json.dumps(flow))
        
    print(f"\n>>> è¯·æ‰“å¼€æµè§ˆå™¨è®¿é—®: {flow['verification_uri']}")
    print(f">>> è¾“å…¥æ­¤ä»£ç : {flow['user_code']}")
    print(">>> ç­‰å¾…ç™»å½•...\n")
    
    # å¢åŠ è¶…æ—¶æ—¶é—´å’Œé‡è¯•
    max_retries = 3
    for i in range(max_retries):
        try:
            result = app.acquire_token_by_device_flow(flow)
            if "access_token" in result:
                return result['access_token']
            else:
                if "authorization_pending" in str(result):
                    continue
                print(f"é”™è¯¯: {result.get('error')}")
                print(f"æè¿°: {result.get('error_description')}")
                sys.exit(1)
        except Exception as e:
            print(f"[ç™»å½•è¿æ¥é”™è¯¯] {str(e)} - æ­£åœ¨é‡è¯• ({i+1}/{max_retries})...")
            time.sleep(2)
            
    print("ç™»å½•è¶…æ—¶æˆ–ç½‘ç»œä¸­æ–­ï¼Œè¯·æ£€æŸ¥ç½‘ç»œåé‡è¯•ã€‚")
    sys.exit(1)

def sanitize_filename(name):
    # æ›¿æ¢éæ³•å­—ç¬¦
    return re.sub(r'[\\/*?:"<>|]', '_', name).strip()

def fetch_json(url, token, retries=5, use_pagination=False):
    headers = {'Authorization': 'Bearer ' + token}
    all_items = []
    
    # å¦‚æœå¼€å¯åˆ†é¡µï¼Œä¸” URL é‡Œæ²¡æœ‰ top å‚æ•°ï¼Œå¼ºåˆ¶åŠ ä¸Š top=20
    # æ³¨æ„ï¼šåªå¯¹è·å–åˆ—è¡¨çš„æ¥å£ç”Ÿæ•ˆï¼ˆnotebooks, sections, pagesï¼‰
    if use_pagination and "$top" not in url:
        separator = "&" if "?" in url else "?"
        url = f"{url}{separator}$top=20"

    current_url = url
    while current_url:
        data = None
        for i in range(retries):
            try:
                response = requests.get(current_url, headers=headers)
                if response.status_code == 200:
                    data = response.json()
                    break
                elif response.status_code == 429: # Too Many Requests
                    wait_time = int(response.headers.get('Retry-After', 10))
                    print(f"      [429 é™æµ] ç­‰å¾… {wait_time} ç§’...")
                    time.sleep(wait_time)
                    continue
                elif response.status_code >= 500: # Server Error
                    print(f"      [æœåŠ¡å™¨é”™è¯¯ {response.status_code}] é‡è¯• ({i+1}/{retries})...")
                    time.sleep(2 ** i) # æŒ‡æ•°é€€é¿
                    continue
                else:
                    print(f"      [API å¤±è´¥] {response.status_code} - {response.text[:100]}...")
                    return None
            except requests.exceptions.RequestException as e:
                print(f"      [ç½‘ç»œé”™è¯¯] {str(e)}ï¼Œé‡è¯•ä¸­ ({i+1}/{retries})...")
                time.sleep(2 ** i)
                continue
        
        if not data:
            print("      [å¤±è´¥] å¤šæ¬¡é‡è¯•åæ— æ³•è·å–æ•°æ®")
            return None if not all_items else {'value': all_items}

        if 'value' in data:
            all_items.extend(data['value'])
            # æ£€æŸ¥æœ‰æ²¡æœ‰ä¸‹ä¸€é¡µ
            if '@odata.nextLink' in data:
                current_url = data['@odata.nextLink']
                print(f"      [åˆ†é¡µ] è·å–ä¸‹ä¸€é¡µæ•°æ®... (å·²è·å– {len(all_items)} æ¡)")
            else:
                current_url = None
        else:
            # å¦‚æœä¸æ˜¯åˆ—è¡¨ç»“æ„ï¼ˆæ¯”å¦‚è·å–å•ä¸ªèµ„æºï¼‰ï¼Œç›´æ¥è¿”å›
            return data

    return {'value': all_items}

def download_file(url, save_path, token, retries=3):
    headers = {'Authorization': 'Bearer ' + token}
    for i in range(retries):
        try:
            response = requests.get(url, headers=headers, stream=True)
            if response.status_code == 200:
                with open(save_path, 'wb') as f:
                    for chunk in response.iter_content(chunk_size=8192):
                        f.write(chunk)
                return True
            elif response.status_code == 429:
                wait_time = int(response.headers.get('Retry-After', 10))
                time.sleep(wait_time)
                continue
            elif response.status_code >= 500:
                time.sleep(2 ** i)
                continue
        except Exception:
            time.sleep(2 ** i)
            continue
    return False

def process_page_content(page_id, token, assets_dir, retries=3):
    # ä½¿ç”¨ includeInkML=true è·å–å¢¨è¿¹ä¿¡æ¯ï¼ˆè™½ç„¶ä¸»è¦è¿˜æ˜¯é  img æ ‡ç­¾ï¼‰
    url = f"https://graph.microsoft.com/v1.0/me/onenote/pages/{page_id}/content?includeIDs=true&includeInkML=true"
    headers = {'Authorization': 'Bearer ' + token}
    
    html_content = None
    for i in range(retries):
        try:
            response = requests.get(url, headers=headers)
            if response.status_code == 200:
                html_content = response.text
                break
            elif response.status_code == 429:
                wait_time = int(response.headers.get('Retry-After', 10))
                print(f"      [429 é™æµ] ç­‰å¾… {wait_time} ç§’...")
                time.sleep(wait_time)
                continue
            elif response.status_code >= 500:
                time.sleep(2 ** i)
                continue
        except requests.exceptions.RequestException as e:
            print(f"      [ç½‘ç»œé”™è¯¯] {str(e)}ï¼Œé‡è¯•ä¸­...")
            time.sleep(2 ** i)
            continue
            
    if not html_content:
        return None

    # è§£æ HTML å¤„ç†å›¾ç‰‡å’Œå¢¨è¿¹
    soup = BeautifulSoup(html_content, 'html.parser')
    
    # åˆ›å»º assets ç›®å½•
    if not os.path.exists(assets_dir):
        os.makedirs(assets_dir)

    # æŸ¥æ‰¾æ‰€æœ‰å›¾ç‰‡ (img) å’Œå¯¹è±¡ (object)
    # OneNote çš„å¢¨è¿¹é€šå¸¸ä¼šä»¥ <img data-src="..." /> æˆ– <object data="..." /> çš„å½¢å¼å­˜åœ¨
    media_tags = soup.find_all(['img', 'object'])
    
    for idx, tag in enumerate(media_tags):
        # è·å–ä¸‹è½½é“¾æ¥
        # data-fullres-src æ˜¯é«˜æ¸…å›¾ï¼Œsrc æ˜¯æ™®é€šå›¾
        src = tag.get('data-fullres-src') or tag.get('src') or tag.get('data')
        
        if not src or not src.startswith('http'):
            continue

        # åˆ¤æ–­æ˜¯å¦ä¸ºé™„ä»¶ (Attachment)
        attachment_name = tag.get('data-attachment')
        is_attachment = bool(attachment_name)
            
        # ç”Ÿæˆæ–‡ä»¶å
        if is_attachment:
            # å¦‚æœæ˜¯é™„ä»¶ï¼Œä¼˜å…ˆä½¿ç”¨åŸæ–‡ä»¶å
            filename = sanitize_filename(attachment_name)
            # é˜²æ­¢æ–‡ä»¶åå†²çªï¼ŒåŠ ä¸ª ID å‰ç¼€
            filename = f"{page_id}_{filename}"
        else:
            # å›¾ç‰‡/å¢¨è¿¹é€»è¾‘ä¿æŒä¸å˜
            ext = '.png' 
            if 'image/jpeg' in str(tag): ext = '.jpg'
            elif 'application/pdf' in str(tag): ext = '.pdf' # æŸäº› PDF æ‰“å°ä»¶
            filename = f"{page_id}_asset_{idx}{ext}"

        save_path = os.path.join(assets_dir, filename)
        
        # ä¸‹è½½æ–‡ä»¶ (å¦‚æœå·²å­˜åœ¨ä¸”å¤§å°ä¸ä¸º0ï¼Œå¯ä»¥è·³è¿‡ä¸‹è½½ï¼Œè¿™é‡Œç®€å•è¦†ç›–)
        if download_file(src, save_path, token):
            # æ›¿æ¢ HTML ä¸­çš„é“¾æ¥ä¸ºç›¸å¯¹è·¯å¾„
            local_rel_path = f"assets/{filename}"
            
            if is_attachment:
                # å¦‚æœæ˜¯é™„ä»¶ï¼Œæ›¿æ¢ä¸ºä¸€ä¸ª Markdown é“¾æ¥ï¼š [æ–‡ä»¶å](è·¯å¾„)
                # å› ä¸º markdownify ä¸ä¼šè‡ªåŠ¨å¤„ç† object ä¸ºé“¾æ¥ï¼Œæˆ‘ä»¬éœ€è¦æ‰‹åŠ¨æŠŠ object æ¢æˆ a æ ‡ç­¾
                new_link = soup.new_tag("a", href=local_rel_path)
                new_link.string = f"ğŸ“ é™„ä»¶: {attachment_name}"
                tag.replace_with(new_link)
            elif tag.name == 'img':
                tag['src'] = local_rel_path
                # ç§»é™¤ data-src é˜²æ­¢å¹²æ‰°
                if tag.has_attr('data-fullres-src'): del tag['data-fullres-src']
            elif tag.name == 'object':
                # å¯¹äºéé™„ä»¶çš„ object (å¯èƒ½æ˜¯ PDF æ‰“å°ä»¶æˆ–å¢¨è¿¹)ï¼Œè½¬ä¸º img
                new_img = soup.new_tag("img")
                new_img['src'] = local_rel_path
                tag.replace_with(new_img)
                
    return str(soup)

def main():
    if not os.path.exists(EXPORT_DIR):
        os.makedirs(EXPORT_DIR)
        
    print("æ­£åœ¨è·å–è®¿é—®ä»¤ç‰Œ...")
    token = get_access_token()
    print("æˆåŠŸè·å–ä»¤ç‰Œï¼å¼€å§‹æ‰«æç¬”è®°æœ¬...")
    
    # 1. è·å–ç¬”è®°æœ¬
    notebooks_url = "https://graph.microsoft.com/v1.0/me/onenote/notebooks"
    notebooks_data = fetch_json(notebooks_url, token)
    
    if not notebooks_data or 'value' not in notebooks_data:
        print("æœªæ‰¾åˆ°ç¬”è®°æœ¬æˆ–æƒé™ä¸è¶³ã€‚")
        return

    for nb in notebooks_data['value']:
        nb_name = sanitize_filename(nb['displayName'])
        print(f"\nå¤„ç†ç¬”è®°æœ¬: {nb_name}")
        nb_path = os.path.join(EXPORT_DIR, nb_name)
        if not os.path.exists(nb_path):
            os.makedirs(nb_path)
            
        # 2. è·å–åˆ†åŒº (Sections)
        sections_url = f"https://graph.microsoft.com/v1.0/me/onenote/notebooks/{nb['id']}/sections"
        sections_data = fetch_json(sections_url, token)
        
        if not sections_data:
            continue
            
        all_sections = sections_data.get('value', [])

        for sec in all_sections:
            sec_name = sanitize_filename(sec['displayName'])
            print(f"  > å¤„ç†åˆ†åŒº: {sec_name}")
            sec_path = os.path.join(nb_path, sec_name)
            assets_path = os.path.join(sec_path, "assets") # æ¯ä¸ªåˆ†åŒºä¸€ä¸ª assets æ–‡ä»¶å¤¹
            
            if not os.path.exists(sec_path):
                os.makedirs(sec_path)
                
            # 3. è·å–é¡µé¢ (Pages)
            # $top=20 ä¸”åªé€‰æ‹© id,title å­—æ®µï¼Œæå¤§å¹…åº¦é™ä½ API è´Ÿè½½ï¼Œé¿å… 504
            pages_url = f"https://graph.microsoft.com/v1.0/me/onenote/sections/{sec['id']}/pages?$top=20&$select=id,title"
            pages_data = fetch_json(pages_url, token, use_pagination=True)
            
            if not pages_data or 'value' not in pages_data:
                continue
                
            all_pages = pages_data['value']
            
            if not all_pages:
                print("    [æç¤º] æœªå‘ç°ç¬”è®°æˆ–è·å–å¤±è´¥")
                continue
                
            for page in all_pages:
                page_title = sanitize_filename(page['title'])
                if not page_title:
                    page_title = f"Untitled_{page['id']}"
                    
                # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
                md_file_path = os.path.join(sec_path, f"{page_title}.md")
                assets_dir = os.path.join(sec_path, "assets")
                
                # æ£€æŸ¥è§„åˆ™ï¼š
                # 1. å¦‚æœ MD æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè‚¯å®šè¦ä¸‹è½½
                # 2. å¦‚æœ MD æ–‡ä»¶å­˜åœ¨ï¼Œä½†å†…å®¹é‡Œæœ‰ "http" å¼€å¤´çš„å›¾ç‰‡é“¾æ¥ï¼ˆè¯´æ˜ä¸Šæ¬¡æ²¡ä¸‹å®Œå›¾ç‰‡ï¼‰ï¼Œä¹Ÿè¦é‡æ–°ä¸‹è½½
                # 3. å¦‚æœ MD æ–‡ä»¶å­˜åœ¨ä¸”å›¾ç‰‡éƒ½æ˜¯æœ¬åœ°é“¾æ¥ï¼Œåˆ™è·³è¿‡
                should_download = True
                
                if os.path.exists(md_file_path):
                    try:
                        with open(md_file_path, 'r', encoding='utf-8') as f:
                            content = f.read()
                            # ç®€å•çš„åˆ¤æ–­ï¼šå¦‚æœå†…å®¹é‡Œæ²¡æœ‰ graph.microsoft.com çš„å›¾ç‰‡é“¾æ¥ï¼Œè¯´æ˜å¯èƒ½å·²ç»å¤„ç†å¥½äº†
                            # æˆ–è€…æ›´ä¸¥æ ¼ï¼šæ£€æŸ¥ assets ç›®å½•é‡Œæœ‰æ²¡æœ‰å¯¹åº”å›¾ç‰‡
                            if "graph.microsoft.com" not in content and os.path.exists(assets_dir):
                                print(f"    [è·³è¿‡] å·²å®Œæ•´: {page_title}")
                                should_download = False
                            else:
                                print(f"    [è¡¥å…¨] å‘ç°æœªæœ¬åœ°åŒ–å›¾ç‰‡: {page_title}")
                    except Exception:
                        pass # è¯»å–é”™è¯¯åˆ™é‡æ–°ä¸‹è½½
                
                if not should_download:
                    continue

                print(f"    - ä¸‹è½½é¡µé¢: {page_title}")
                
                # ä¸‹è½½å¤„ç†åçš„ HTML å†…å®¹ï¼ˆåŒ…å«å›¾ç‰‡ä¸‹è½½é€»è¾‘ï¼‰
                try:
                    processed_html = process_page_content(page['id'], token, assets_path)
                except Exception as e:
                    print(f"      [é”™è¯¯] å¤„ç†å¤±è´¥: {str(e)}")
                    continue
                    
                if processed_html:
                    # è½¬æ¢ä¸º Markdown
                    markdown_content = md(processed_html)
                    
                    # ä¿å­˜æ–‡ä»¶
                    with open(md_file_path, 'w', encoding='utf-8') as f:
                        f.write(markdown_content)
                else:
                    print(f"      [å¤±è´¥] æ— æ³•ä¸‹è½½å†…å®¹")

    print(f"\næ‰€æœ‰å®Œæˆï¼ç¬”è®°å·²ä¿å­˜åœ¨ {os.path.abspath(EXPORT_DIR)}")

if __name__ == '__main__':
    main()
